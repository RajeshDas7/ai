{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e16ef5",
   "metadata": {},
   "source": [
    "### 1. What is feature engineering, and how does it work? Explain the various aspects of feature\n",
    "engineering in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c773aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature engineering is the process of using domain knowledge to\n",
    "extract features (characteristics, properties, attributes) from raw data.\n",
    "\n",
    "Feature Selection\n",
    "Handling missing values\n",
    "Handling imbalanced data\n",
    "Handling outliers\n",
    "Binning\n",
    "Encoding\n",
    "Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f2619",
   "metadata": {},
   "source": [
    "### 2. What is feature selection, and how does it work? What is the aim of it? What are the various\n",
    "methods of function selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "The goal of feature selection in machine learning is to find the best\n",
    "set of features that allows one to build useful models of studied phenomena.\n",
    "\n",
    "\n",
    "Supervised Techniques: These techniques can be used for labeled data, and are used to identify the relevant features for increasing the efficiency of supervised models like classification and regression.\n",
    "\n",
    "Unsupervised Techniques: These techniques can be used for unlabeled data.\n",
    "\n",
    "From a taxonomic point of view, these techniques are classified as under:\n",
    "\n",
    "A. Filter methods\n",
    "\n",
    "B. Wrapper methods\n",
    "\n",
    "C. Embedded methods\n",
    "\n",
    "D. Hybrid methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be7cb9",
   "metadata": {},
   "source": [
    "### 3. Describe the function selection filter and wrapper approaches. State the pros and cons of each\n",
    "approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842965a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filter methods are generally used as a preprocessing step. The selection of features\n",
    "is independent of any machine learning algorithms. Instead, features are selected on the basis of their \n",
    "scores in various statistical tests for their correlation with the outcome variable.\n",
    "\n",
    "In wrapper methods, we try to use a subset of features and train a model using them.\n",
    "Based on the inferences that we draw from the previous model, we decide to \n",
    "add or remove features from your subset. The problem is essentially reduced to a search problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22322a61",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "i. Describe the overall feature selection process.\n",
    "\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most\n",
    "widely used function extraction algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981acc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filter methods\n",
    "Wrapper methods\n",
    "Embedded methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d4e050",
   "metadata": {},
   "source": [
    "### 5. Describe the feature engineering process in the sense of a text categorization issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5913b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2309dba",
   "metadata": {},
   "source": [
    "### 6. What makes cosine similarity a good metric for text categorization? A document-term matrix has\n",
    "two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in\n",
    "cosine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb3e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f0d29d1",
   "metadata": {},
   "source": [
    "### 7.\n",
    "\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
    "calculate the Hamming gap.\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,\n",
    "0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be851af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if character is similar 1 otherwise 0\n",
    " 10001011 and 11001111 hamming distance =2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4268ffeb",
   "metadata": {},
   "source": [
    "### 8. State what is meant by &quot;high-dimensional data set&quot;? Could you offer a few real-life examples?\n",
    "What are the difficulties in using machine learning techniques on a data set with many dimensions?\n",
    "What can be done about it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbce107",
   "metadata": {},
   "outputs": [],
   "source": [
    "High dimensional data refers to a dataset in which the number of features p is\n",
    "larger than the number of observations N, often written as p >> N.\n",
    "\n",
    "For example, a dataset that has p = 6 features and only N = 3 observations would be considered high dimensional data because the number of features is \n",
    "larger than the number of observations.\n",
    "\n",
    "When the number of features in a dataset exceeds the number of observations, we will never have a deterministic answer.\n",
    "\n",
    "In other words, it becomes impossible to find a model that can describe the relationship between the predictor variables and the \n",
    "response variable because we don’t have enough observations to train the model on.\n",
    "\n",
    "High dimensional data is common in healthcare datasets where the number of features for \n",
    "a given individual can be massive (i.e. blood pressure, resting heart rate, immune system status, \n",
    "surgery history, height, weight, existing conditions, etc.).\n",
    "\n",
    "In these datasets, it’s common for the number of features to be larger than the number of observations.\n",
    "\n",
    "There are two common ways to deal with high dimensional data:\n",
    "\n",
    "1. Choose to include fewer features\n",
    "2. Use a regularization method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7073c753",
   "metadata": {},
   "source": [
    "### 9. Make a few quick notes on:\n",
    "\n",
    "PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "2. Use of vectors\n",
    "\n",
    "3. Embedded technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccabf1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db58a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "10. Make a comparison between:\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "\n",
    "3. SMC vs. Jaccard coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filter methods are generally used as a preprocessing step. The selection of features\n",
    "is independent of any machine learning algorithms. Instead, features are selected on the basis of their \n",
    "scores in various statistical tests for their correlation with the outcome variable.\n",
    "\n",
    "In wrapper methods, we try to use a subset of features and train a model using them.\n",
    "Based on the inferences that we draw from the previous model, we decide to \n",
    "add or remove features from your subset. The problem is essentially reduced to a search problem.\n",
    "\n",
    "\n",
    "The main difference is that the SMC has the term in its numerator\n",
    "and denominator, whereas the Jaccard index does not. Thus, the SMC counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent in both sets) as matches and compares it to the total number of attributes in the universe, whereas the Jaccard index only counts mutual presence\n",
    "as matches and compares it to the number of attributes that have been chosen by at least one of the two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694e6408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a813d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df69aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589b0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45456f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125ecf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
